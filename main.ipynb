{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, we are going to replicate the VolGAN-example file, by demonstrating the modifications we are making to the VolGAN file and introducing new functions we have created.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#import files with supporting functions\n",
    "import VolGANSwaps as VGS\n",
    "import Inputs\n",
    "\n",
    "#import our data sets and a sample date for testing\n",
    "VOLATILITY_DATA = \"data/swaption_atm_vol_full_NEW_no30y.xlsx\"\n",
    "FORWARDS_DATA = \"data/forward_sofr_swap_full_NEW_no30y.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**\n",
    "- The data loader takes in the filepath to the volatility surface and the forward swap rates\n",
    "- Constructs returns on underlying\n",
    "- Processes the volatility data\n",
    "    - The surface for every underlying asset is represented as a vector\n",
    "    - This is similarly the case for the returns\n",
    "- Sets up the tenor and tau matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfaces_transform, returns, tenor, tau, tenors, taus, dates_dt = VGS.SwapsData(FORWARDS_DATA, VOLATILITY_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "- Within `DataPreprocessing` we call `SwapsData` like the original VolGAN implementation\n",
    "- Here we construct the condition vector, construct the true labels and pass through some other data that might be relevant\n",
    "    - Each underlying has a condition vector and the true labels (annualized return and log implied vol increment)\n",
    "    - Because of this they will actually end up being tensor inputs to the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing example\n",
    "\n",
    "true, condition, m_in,sigma_in, m_out, sigma_out, dates_t,  tenor, tau, tenors, taus = VGS.DataPreprocesssing(FORWARDS_DATA, VOLATILITY_DATA, vol_model='normal')\n",
    "# true.shape\n",
    "# condition.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Train Test Split**\n",
    "- Separates dataset into train and test datasets after processing and loading\n",
    "- Parameterized by datasize percentage / proportion\n",
    "- Parameterized by a data transformation assumption (`normal` or `log`)\n",
    "    - Not directly related to volatility model assumptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train, true_test, condition_train,  condition_test,  m_in,sigma_in, m_out, sigma_out, dates_t,  tenor, tau, tenors, taus = VGS.DataTrainTest(FORWARDS_DATA, VOLATILITY_DATA, 0.8, vol_model='normal')\n",
    "# true_train.shape\n",
    "# true_test.shape\n",
    "# condition_train.shape\n",
    "# condition_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we are going to run the model.**\n",
    "- We updated the two training loops in the paper by ensuring the dimensionality of the computations were correct\n",
    "- We also accounted for the fact that we are using the normal model\n",
    "- If desired, we can still utilize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 0.85\n",
    "noise_dim = 32\n",
    "hidden_dim = 16\n",
    "n_epochs = 10000\n",
    "n_grad = 25\n",
    "val = True\n",
    "vol_model = 'normal'\n",
    "test_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be trained on either gpu or cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present training for a low number of epochs to ensure that the loops work.\n",
    "- Output of this volgan function provides us set of data used to train the models\n",
    "- Also `gen` and `disc` are the trained models that we can then evaluate once trained for more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [00:07<00:01,  2.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen, gen_opt, disc, disc_opt, true_train, true_val, true_test, condition_train, condition_val, condition_test, dates_t,  tenor, tau, tenors, taus  \u001b[38;5;241m=\u001b[39m VGS\u001b[38;5;241m.\u001b[39mVolGAN(FORWARDS_DATA,VOLATILITY_DATA, tr, noise_dim \u001b[38;5;241m=\u001b[39m noise_dim, hidden_dim \u001b[38;5;241m=\u001b[39m hidden_dim, n_epochs \u001b[38;5;241m=\u001b[39m test_epoch,n_grad \u001b[38;5;241m=\u001b[39m n_grad, lrg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m, lrd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, device \u001b[38;5;241m=\u001b[39m device, vol_model\u001b[38;5;241m=\u001b[39mvol_model)\n",
      "File \u001b[1;32mc:\\Users\\erict\\GitRepositories\\projectlab\\VolGAN\\VolGANSwaps.py:372\u001b[0m, in \u001b[0;36mVolGAN\u001b[1;34m(datapath, surfacepath, tr, vol_model, noise_dim, hidden_dim, n_epochs, n_grad, lrg, lrd, batch_size, device)\u001b[0m\n\u001b[0;32m    369\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m    370\u001b[0m criterion \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 372\u001b[0m gen,gen_opt,disc,disc_opt,criterion, alpha, beta \u001b[38;5;241m=\u001b[39m GradientMatching(gen,gen_opt,disc,disc_opt,criterion,condition_train,true_train,tenor,tau,tenors,taus,n_grad,lrg,lrd,batch_size,noise_dim,device, vol_model\u001b[38;5;241m=\u001b[39mvol_model)\n\u001b[0;32m    373\u001b[0m gen,gen_opt,disc,disc_opt,criterion \u001b[38;5;241m=\u001b[39m TrainLoopNoVal(alpha,beta,gen,gen_opt,disc,disc_opt,criterion,condition_train,true_train,tenor,tau,tenors,taus,n_epochs,lrg,lrd,batch_size,noise_dim,device, vol_model\u001b[38;5;241m=\u001b[39mvol_model)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen, gen_opt, disc, disc_opt, true_train, true_val, true_test, condition_train, condition_val, condition_test, dates_t,  tenor, tau, tenors, taus\n",
      "File \u001b[1;32mc:\\Users\\erict\\GitRepositories\\projectlab\\VolGAN\\VolGANSwaps.py:504\u001b[0m, in \u001b[0;36mGradientMatching\u001b[1;34m(gen, gen_opt, disc, disc_opt, criterion, condition_train, true_train, tenor, tau, tenors, taus, n_grad, lrg, lrd, batch_size, noise_dim, device, lk, lt, vol_model)\u001b[0m\n\u001b[0;32m    501\u001b[0m tenor_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(penalties_tenor) \u001b[38;5;241m/\u001b[39m curr_batch_size\n\u001b[0;32m    502\u001b[0m t_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(penalties_t) \u001b[38;5;241m/\u001b[39m curr_batch_size\n\u001b[1;32m--> 504\u001b[0m tenor_penalty\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    505\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m gen\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[1;32mc:\\Users\\erict\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\erict\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\erict\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen, gen_opt, disc, disc_opt, true_train, true_val, true_test, condition_train, condition_val, condition_test, dates_t,  tenor, tau, tenors, taus  = VGS.VolGAN(FORWARDS_DATA,VOLATILITY_DATA, tr, noise_dim = noise_dim, hidden_dim = hidden_dim, n_epochs = test_epoch,n_grad = n_grad, lrg = 0.0001, lrd = 0.0001, batch_size = 100, device = device, vol_model=vol_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results from the Gradient Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VOLATILITY_DATA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m lrd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m \n\u001b[0;32m     10\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \n\u001b[1;32m---> 12\u001b[0m true_train, true_test, condition_train, condition_test,  m_in,sigma_in, m_out, sigma_out, dates_t,  tenor, tau, tenors, taus \u001b[38;5;241m=\u001b[39m VGS\u001b[38;5;241m.\u001b[39mDataTrainTest(FORWARDS_DATA, VOLATILITY_DATA, tr, vol_model, device)\n\u001b[0;32m     13\u001b[0m gen \u001b[38;5;241m=\u001b[39m VGS\u001b[38;5;241m.\u001b[39mGenerator(noise_dim\u001b[38;5;241m=\u001b[39mnoise_dim,cond_dim\u001b[38;5;241m=\u001b[39mcondition_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim,output_dim\u001b[38;5;241m=\u001b[39mtrue_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],mean_in \u001b[38;5;241m=\u001b[39m m_in, std_in \u001b[38;5;241m=\u001b[39m sigma_in, mean_out \u001b[38;5;241m=\u001b[39m m_out, std_out \u001b[38;5;241m=\u001b[39m sigma_out)\n\u001b[0;32m     14\u001b[0m gen\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VOLATILITY_DATA' is not defined"
     ]
    }
   ],
   "source": [
    "tr = 0.85\n",
    "noise_dim = 32\n",
    "hidden_dim = 16\n",
    "n_epochs = 10000\n",
    "n_grad = 250\n",
    "val = True\n",
    "vol_model = 'normal'\n",
    "lrg = 0.0001 \n",
    "lrd = 0.0001 \n",
    "batch_size = 100 \n",
    "\n",
    "true_train, true_test, condition_train, condition_test,  m_in,sigma_in, m_out, sigma_out, dates_t,  tenor, tau, tenors, taus = VGS.DataTrainTest(FORWARDS_DATA, VOLATILITY_DATA, tr, vol_model, device)\n",
    "gen = VGS.Generator(noise_dim=noise_dim,cond_dim=condition_train.shape[2], hidden_dim=hidden_dim,output_dim=true_train.shape[2],mean_in = m_in, std_in = sigma_in, mean_out = m_out, std_out = sigma_out)\n",
    "gen.to(device)\n",
    "\n",
    "# m_disc and sigma_disc are not used in the original VolGAN, you can see in the Discriminator forward function \n",
    "# we'll preserve the forward pass for now but we can maybe incorporate these later if needed    \n",
    "m_disc = torch.cat((m_in,m_out),dim=-1)\n",
    "sigma_disc = torch.cat((sigma_in,sigma_out),dim=-1)\n",
    "\n",
    "disc = VGS.Discriminator(in_dim = condition_train.shape[2] + true_train.shape[2], hidden_dim = hidden_dim, mean = m_disc, std = sigma_disc)\n",
    "disc.to(device)\n",
    "\n",
    "true_val = False\n",
    "condition_val = False\n",
    "\n",
    "gen_opt = torch.optim.RMSprop(gen.parameters(), lr=lrg)\n",
    "disc_opt = torch.optim.RMSprop(disc.parameters(), lr=lrd)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 33/250 [00:11<01:14,  2.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen,gen_opt,disc,disc_opt,criterion, alpha, beta \u001b[38;5;241m=\u001b[39m VGS\u001b[38;5;241m.\u001b[39mGradientMatchingPlot(gen,gen_opt,disc,disc_opt,criterion,condition_train,true_train,tenor,tau,tenors,taus,n_grad,lrg,lrd,batch_size,noise_dim,device, vol_model\u001b[38;5;241m=\u001b[39mvol_model)\n",
      "File \u001b[1;32mc:\\Users\\erict\\GitRepositories\\projectlab\\VolGAN\\VolGANSwaps.py:638\u001b[0m, in \u001b[0;36mGradientMatchingPlot\u001b[1;34m(gen, gen_opt, disc, disc_opt, criterion, condition_train, true_train, tenor, tau, tenors, taus, n_grad, lrg, lrd, batch_size, noise_dim, device, lk, lt, vol_model)\u001b[0m\n\u001b[0;32m    635\u001b[0m tenor_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(penalties_tenor) \u001b[38;5;241m/\u001b[39m curr_batch_size\n\u001b[0;32m    636\u001b[0m t_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(penalties_t) \u001b[38;5;241m/\u001b[39m curr_batch_size\n\u001b[1;32m--> 638\u001b[0m tenor_penalty\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    639\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m gen\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    640\u001b[0m tenor_smooth_grad\u001b[38;5;241m.\u001b[39mappend(total_norm)\n",
      "File \u001b[1;32mc:\\Users\\erict\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\erict\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\erict\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen,gen_opt,disc,disc_opt,criterion, alpha, beta = VGS.GradientMatchingPlot(gen,gen_opt,disc,disc_opt,criterion,condition_train,true_train,tenor,tau,tenors,taus,n_grad,lrg,lrd,batch_size,noise_dim,device, vol_model=vol_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full training with 10,000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:26<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 3.6221621464789517e-06 beta : 0.0005788063762053093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1508/10000 [08:05<45:35,  3.10it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen, gen_opt, disc, disc_opt, true_train, true_val, true_test, condition_train, condition_val, condition_test, dates_t,  tenor, tau, tenors, taus  \u001b[38;5;241m=\u001b[39m VGS\u001b[38;5;241m.\u001b[39mVolGAN(DATA_PATH,SURFACE_PATH, tr, noise_dim \u001b[38;5;241m=\u001b[39m noise_dim, hidden_dim \u001b[38;5;241m=\u001b[39m hidden_dim, n_epochs \u001b[38;5;241m=\u001b[39m n_epochs,n_grad \u001b[38;5;241m=\u001b[39m n_grad, lrg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m, lrd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, device \u001b[38;5;241m=\u001b[39m device, vol_model\u001b[38;5;241m=\u001b[39mvol_model)\n",
      "File \u001b[1;32mc:\\Users\\erict\\GitRepositories\\projectlab\\VolGAN\\VolGANSwaps.py:373\u001b[0m, in \u001b[0;36mVolGAN\u001b[1;34m(datapath, surfacepath, tr, vol_model, noise_dim, hidden_dim, n_epochs, n_grad, lrg, lrd, batch_size, device)\u001b[0m\n\u001b[0;32m    370\u001b[0m criterion \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    372\u001b[0m gen,gen_opt,disc,disc_opt,criterion, alpha, beta \u001b[38;5;241m=\u001b[39m GradientMatching(gen,gen_opt,disc,disc_opt,criterion,condition_train,true_train,tenor,tau,tenors,taus,n_grad,lrg,lrd,batch_size,noise_dim,device, vol_model\u001b[38;5;241m=\u001b[39mvol_model)\n\u001b[1;32m--> 373\u001b[0m gen,gen_opt,disc,disc_opt,criterion \u001b[38;5;241m=\u001b[39m TrainLoopNoVal(alpha,beta,gen,gen_opt,disc,disc_opt,criterion,condition_train,true_train,tenor,tau,tenors,taus,n_epochs,lrg,lrd,batch_size,noise_dim,device, vol_model\u001b[38;5;241m=\u001b[39mvol_model)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen, gen_opt, disc, disc_opt, true_train, true_val, true_test, condition_train, condition_val, condition_test, dates_t,  tenor, tau, tenors, taus\n",
      "File \u001b[1;32mc:\\Users\\erict\\GitRepositories\\projectlab\\VolGAN\\VolGANSwaps.py:784\u001b[0m, in \u001b[0;36mTrainLoopNoVal\u001b[1;34m(alpha, beta, gen, gen_opt, disc, disc_opt, criterion, condition_train, true_train, tenor, tau, tenors, taus, n_epochs, lrg, lrd, batch_size, noise_dim, device, lk, lt, vol_model)\u001b[0m\n\u001b[0;32m    782\u001b[0m     penalties_t[iii] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(tsq,(torch\u001b[38;5;241m.\u001b[39mmatmul(matrix_t,fake_surface[iii])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    783\u001b[0m m_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(penalties_m) \u001b[38;5;241m/\u001b[39m curr_batch_size\n\u001b[1;32m--> 784\u001b[0m t_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(penalties_t) \u001b[38;5;241m/\u001b[39m curr_batch_size\n\u001b[0;32m    786\u001b[0m gen_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    787\u001b[0m gen_loss \u001b[38;5;241m=\u001b[39m criterion(disc_fake_pred, torch\u001b[38;5;241m.\u001b[39mones_like(disc_fake_pred)) \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m m_penalty \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m t_penalty\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen, gen_opt, disc, disc_opt, true_train, true_val, true_test, condition_train, condition_val, condition_test, dates_t,  tenor, tau, tenors, taus  = VGS.VolGAN(FORWARDS_DATA,VOLATILITY_DATA, tr, noise_dim = noise_dim, hidden_dim = hidden_dim, n_epochs = n_epochs,n_grad = n_grad, lrg = 0.0001, lrd = 0.0001, batch_size = 100, device = device, vol_model=vol_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving and Loading Trained Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the model\n",
    "\n",
    "gen_filepath = 'model/generator_parameters.pth'\n",
    "disc_filepath = 'model/discriminator_parameters.pth'\n",
    "\n",
    "torch.save(gen.state_dict(), gen_filepath)\n",
    "torch.save(disc.state_dict(), disc_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model  \n",
    "\n",
    "tr = 0.85\n",
    "noise_dim = 32\n",
    "hidden_dim = 16\n",
    "n_epochs = 10000\n",
    "n_grad = 250\n",
    "val = True\n",
    "vol_model = 'normal'\n",
    "lrg = 0.0001 \n",
    "lrd = 0.0001 \n",
    "batch_size = 100 \n",
    "\n",
    "gen = VGS.Generator(noise_dim=noise_dim,cond_dim=condition_train.shape[2], hidden_dim=hidden_dim,output_dim=true_train.shape[2],mean_in = m_in, std_in = sigma_in, mean_out = m_out, std_out = sigma_out)\n",
    "disc = VGS.Discriminator(in_dim = condition_train.shape[2] + true_train.shape[2], hidden_dim = hidden_dim, mean = m_disc, std = sigma_disc)\n",
    "\n",
    "gen.to(device)\n",
    "disc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.load_state_dict(torch.load(gen_filepath))\n",
    "disc.load_state_dict(torch.load(disc_filepath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
